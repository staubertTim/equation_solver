{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d11-9IUd9mbE"
      },
      "source": [
        "## Processing Digits and Symbols "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook serves to create a classifier, that is able to recognize all numbers from 0 to 9 and plus and minus signs. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It was used in the Google Colab environment and got its data from a Google Drive folder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step in the notebook is to load all neccessary imports. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import idx2numpy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "import os\n",
        "import struct\n",
        "import torch\n",
        "import torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then a function is defined, that load  the training images from google drive. These include the MNIST train images as well as some manipulated images from the mathematical symbol dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCMqlRxjCI3K",
        "outputId": "66ca6e64-2470-4b3f-8909-eea74c40eea6"
      },
      "outputs": [],
      "source": [
        "def load_train_images():\n",
        "    path = '/content/drive/MyDrive/dlproject/'\n",
        "\n",
        "    file = path + 'train-images.idx3-ubyte'\n",
        "    images = idx2numpy.convert_from_file(file)\n",
        "    # arr is now a np.ndarray type of object of shape 60000, 28, 28\n",
        "\n",
        "    with open(path + 'train-labels.idx1-ubyte','rb') as f:\n",
        "        magic, size = struct.unpack(\">II\", f.read(8))\n",
        "        data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
        "        labels = data.reshape((size,)) # (Optional)\n",
        "\n",
        "    minuses = os.listdir(f'{path}dataset/minus-imgs/')\n",
        "    img_minuses = []\n",
        "    for p in minuses:\n",
        "      img = image.imread(f'{path}dataset/minus-imgs/{p}')\n",
        "      img_minuses.append(img)\n",
        "      print(f'\\r{p}', end='')\n",
        "\n",
        "    pluses = os.listdir(f'{path}dataset/plus-imgs/')\n",
        "    img_pluses = []\n",
        "    for p in pluses:\n",
        "      img = image.imread(f'{path}dataset/plus-imgs/{p}')\n",
        "      img_pluses.append(img)\n",
        "      print(f'\\r{p}', end='')\n",
        "\n",
        "    x = np.append(images, img_pluses + img_minuses, axis=0)\n",
        "    y = np.append(labels, [10]*len(pluses) + [11]*len(minuses), axis=0)\n",
        "\n",
        "    assert len(x) == len(y)\n",
        "    p = np.random.permutation(len(x))\n",
        "    return x[p], y[p]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This now saves the training images and training labels on Google Drive in numpy format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9R9zVR-C4Dc",
        "outputId": "c9a4ad9c-1a12-404c-fd4c-d1dad365ed32"
      },
      "outputs": [],
      "source": [
        "all_images, all_labels = load_train_images()\n",
        "\n",
        "path = '/content/drive/MyDrive/dlproject/cnn/'\n",
        "np.save(f\"{path}all_images.npy\", all_images)\n",
        "np.save(f\"{path}all_labels.npy\" , all_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To show the data, one random set of image and label will be outputted by the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "NoOaNNEgC_Z5",
        "outputId": "7403c3e7-c1b9-4a92-9aa8-4009a6acea23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "rand = np.random.randint(0, all_labels.shape[0] -1)\n",
        "plt.imshow(all_images[rand])\n",
        "print(all_labels[rand])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next cell will allow for the operations to happen on GPU, if CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the images and labels will be put into the format of a PyTorch Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MXD3hBcPO3F"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, labels, images, transform=None, target_transform=None):\n",
        "        self.labels = labels\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        image = self.images[idx]\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "          label = self.target_transform(label)\n",
        "        return image.to(device), label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the definintion of the Dataset, we can now create a train and a test dataset, where the test dataset consists of 6000 images. We can then also initialize two PyTorch DataLoaders (one for training and one for testing) with a batch size of 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data = CustomImageDataset(all_labels[:-6000], all_images[:-6000], transform=ToTensor())\n",
        "test_data = CustomImageDataset(all_labels[-6000:], all_images[-6000:], transform=ToTensor())\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we can finally define the architecture of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUjbXQQK-V-Q"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):                  #Score: 99.2%\n",
        "    def __init__(self, out_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(     #Input 28*28*1     \n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=0), #Output 24*24*32                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2) #Output 12*12*32\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, tride=1, padding=0), #Output 10*10*64\n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2), #Output 5*5*64              \n",
        "        )\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(5*5*64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, out_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Put through 2 convolution layers and a 3L deep linear network with dropouts\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1) #Flatten\n",
        "        output = self.dense(x)\n",
        "        return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the architecture in place, we can now define how one training loop is supposed to look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5NK7u_g_bM1"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device=device):\n",
        "    model = model.to(device)\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And also how one test loop is supposed to look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model = model.to(device)\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the training and test loop also in place we can now initialize the classifier, define its optimizer and then start to train it for ten epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph7LIi5E-YRj",
        "outputId": "c26e6414-90f9-4e26-d3e6-0673f52f7ffe"
      },
      "outputs": [],
      "source": [
        "model = CNN(12)\n",
        "\n",
        "learning_rate = 1e-3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the test loop, we saw that the accuray was above 97% in the last epoch, which was good enough for our purpose. So we saved the resulting model to use it in the main Project Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_sNLO9USS_q"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '../resources/cnn.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
